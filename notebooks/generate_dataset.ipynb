{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "generate_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_C9b6ZbbjHB"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "import tqdm\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zFhoj9eXnJ-"
      },
      "source": [
        "torch.multiprocessing.set_start_method('spawn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzfKIpz435Z-"
      },
      "source": [
        "# for colab\n",
        "!git clone https://github.com/laralex/Sk-DL2021-FinalProject\n",
        "repo_dir = Path().absolute()/'Sk-DL2021-FinalProject'\n",
        "%pushd Sk-DL2021-FinalProject\n",
        "!git pull\n",
        "!git checkout new_gen\n",
        "!pip install pytorch_lightning\n",
        "import sys\n",
        "sys.path.append('Sk-DL2021-FinalProject')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiHxQlmybjHH"
      },
      "source": [
        "# for local\n",
        "# import sys\n",
        "# sys.path.append('..')\n",
        "# repo_dir = Path().absolute().parent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87lwyusR35aH"
      },
      "source": [
        "!pwd\n",
        "\n",
        "import torch\n",
        "from data.split_step_generator import SplitStepGenerator\n",
        "from auxiliary.files import find_dataset_subdir\n",
        "\n",
        "GOOGLE_DRIVE = True\n",
        "\n",
        "if GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(f'/content/drive')\n",
        "    root_dir = Path('/content/drive/MyDrive/Sk-DL2021-Datasets')\n",
        "else:\n",
        "    root_dir = repo_dir.parent / 'generated_datasets'\n",
        "if not os.path.exists(root_dir):\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    \n",
        "root_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKWY49GzKKaG"
      },
      "source": [
        "import yaml\n",
        "CONFIG_NAME = 'generate_dataset_nonlin0.05-0.5_compensate'\n",
        "CONFIG = repo_dir/'configs'/f\"{CONFIG_NAME}.yaml\"\n",
        "\n",
        "with open(CONFIG, 'r') as stream:\n",
        "    config_hparams = yaml.safe_load(stream)['data']['init_args']\n",
        "# config_hparams['dispersion_compensate'] = True\n",
        "data_gen = SplitStepGenerator(**config_hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzV9UCYdKKaH"
      },
      "source": [
        "import yaml\n",
        "import datetime \n",
        "\n",
        "NEW_DIR_NAME = CONFIG_NAME\n",
        "\n",
        "def create_destination(hparams, datasets_root, new_dir_name=None):\n",
        "    if new_dir_name is None:\n",
        "        new_dir = root_dir/datetime.datetime.now().strftime(\"%m-%d-%Y=%H-%M-%S\")\n",
        "    else:\n",
        "        new_dir = root_dir/new_dir_name\n",
        "    os.makedirs(new_dir, exist_ok=True)\n",
        "    assert not os.path.exists(f'{new_dir}/signal_hparams.yaml')\n",
        "    with open(f'{new_dir}/signal_hparams.yaml', 'w') as outfile:\n",
        "        yaml.dump(hparams, outfile, default_flow_style=False)\n",
        "    return new_dir\n",
        "    \n",
        "destination_root = find_dataset_subdir(data_gen.signal_hparams, root_dir)\n",
        "if destination_root is None:\n",
        "    destination_root = create_destination(data_gen.signal_hparams, root_dir, NEW_DIR_NAME)\n",
        "print('Destination: ', destination_root) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3LplAKJKKaI"
      },
      "source": [
        "# make folders structure\n",
        "def save_tensor(tensor, subdir):\n",
        "    if tensor is None:\n",
        "        print('Nothing to save', subdir)\n",
        "        return\n",
        "    if tensor.numel() == 0:\n",
        "        return\n",
        "    i = 0\n",
        "    while os.path.exists(subdir/f\"{i}.pt\"):\n",
        "        i += 1\n",
        "    destination_path = subdir/f\"{i}.pt\"\n",
        "    torch.save(torch.tensor([]), destination_path)\n",
        "    torch.save(tensor.clone(), destination_path)\n",
        "    \n",
        "type_subdirs = [destination_root/sub for sub in ['train', 'val', 'test']]\n",
        "for d in type_subdirs:\n",
        "    os.makedirs(d, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZOijqqLKKaJ"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZMafEeAbjHJ"
      },
      "source": [
        "import yaml\n",
        "\n",
        "BATCH_SIZE = 20\n",
        "GENERATE_TRAIN_BATCHES = 50\n",
        "GENERATE_VAL_BATCHES = 0\n",
        "GENERATE_TEST_BATCHES = 0\n",
        "\n",
        "MIN_NONLIN = 0.02\n",
        "MAX_NONLIN = 0.4\n",
        "\n",
        "if os.path.exists(CONFIG):\n",
        "    with open(CONFIG, 'r') as stream:\n",
        "        train_hparams = yaml.safe_load(stream)['data']['init_args']\n",
        "        train_hparams['batch_size'] = BATCH_SIZE\n",
        "        train_hparams['generate_n_train_batches'] = GENERATE_TRAIN_BATCHES\n",
        "        train_hparams['generate_n_val_batches'] = GENERATE_VAL_BATCHES\n",
        "        train_hparams['generate_n_test_batches'] = GENERATE_TEST_BATCHES\n",
        "        data_gen = SplitStepGenerator(**train_hparams)\n",
        "        data_gen.prepare_data()\n",
        "else:\n",
        "    print('Config file cant be found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu7yP7JL35aJ"
      },
      "source": [
        "train_hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73_7x0z435aO"
      },
      "source": [
        "# generate and save\n",
        "loader = data_gen.train_dataloader()\n",
        "loader.num_workers = 0\n",
        "for inp, target in tqdm.tqdm(loader):\n",
        "  if len(target.shape) == 4:\n",
        "    target = target.squeeze(0)\n",
        "    inp = inp.squeeze(0)\n",
        "    assert len(target.shape) == 3 and len(inp.shape) == 3\n",
        "  b = torch.stack([target, inp], dim=0)\n",
        "  print(b.shape, b.sum())\n",
        "  save_tensor(b, type_subdirs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LGhEcZhKKaM"
      },
      "source": [
        "VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej8EQ_k7KKaN"
      },
      "source": [
        "import yaml\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "GENERATE_TRAIN_BATCHES = 0\n",
        "GENERATE_VAL_BATCHES = 3\n",
        "GENERATE_TEST_BATCHES = 0\n",
        "\n",
        "NONLINEARITY = 0.4\n",
        "\n",
        "if os.path.exists(CONFIG):\n",
        "    with open(CONFIG, 'r') as stream:\n",
        "        val_hparams = yaml.safe_load(stream)['data']['init_args']\n",
        "        val_hparams['batch_size'] = BATCH_SIZE\n",
        "        val_hparams['generate_n_train_batches'] = GENERATE_TRAIN_BATCHES\n",
        "        val_hparams['generate_n_val_batches'] = GENERATE_VAL_BATCHES\n",
        "        val_hparams['generate_n_test_batches'] = GENERATE_TEST_BATCHES\n",
        "        val_hparams['generation_nonlinearity_limits'] = None\n",
        "        val_hparams['nonlinearity'] = NONLINEARITY\n",
        "else:\n",
        "    print('Config file cant be found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFs4YR3YKKaO"
      },
      "source": [
        "val_hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ljK3kE3gBiD"
      },
      "source": [
        "data_gen = SplitStepGenerator(**val_hparams)\n",
        "data_gen.prepare_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03DHsRzIKKaP"
      },
      "source": [
        "# generate and save\n",
        "loader = data_gen.val_dataloader()\n",
        "loader.num_workers = 0\n",
        "for inp, target in tqdm.tqdm(loader):\n",
        "  if len(target.shape) == 4:\n",
        "    target = target.squeeze(0)\n",
        "    inp = inp.squeeze(0)\n",
        "    assert len(target.shape) == 3 and len(inp.shape) == 3\n",
        "  b = torch.stack([target, inp])\n",
        "  print(b.shape, b.sum())\n",
        "  save_tensor(b, type_subdirs[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j18_FeOaKKaR"
      },
      "source": [
        "LOAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j84w2kkabjHP"
      },
      "source": [
        "config_hparams['data_source_type'] = 'filesystem'\n",
        "config_hparams['load_dataset_root_path'] = root_dir\n",
        "config_hparams['batch_size'] = 20\n",
        "data_gen_load = SplitStepGenerator(**config_hparams)\n",
        "config_hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5QFChzz35aR"
      },
      "source": [
        "data_gen_load.prepare_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1FOv0iekBGM"
      },
      "source": [
        "loader = data_gen_load.train_dataloader()\n",
        "loader.num_workers = 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQbYABtvhchj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for idx, t in enumerate(loader):\n",
        "    fig = plt.figure()\n",
        "    plt.plot(t[0][2, :300, 5].real)\n",
        "    plt.plot(t[1][2, :300, 5].real)\n",
        "    plt.plot(t[0][2, :300, 5].imag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RWNOtCyizrc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}