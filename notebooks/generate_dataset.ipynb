{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzfKIpz435Z-",
    "outputId": "06050e14-8f92-4110-d022-73177b764940"
   },
   "outputs": [],
   "source": [
    "# for colab\n",
    "# !git clone https://github.com/laralex/Sk-DL2021-FinalProject\n",
    "# repo_dir = Path().absolute()/'Sk-DL2021-FinalProject'\n",
    "# %pushd Sk-DL2021-FinalProject\n",
    "# !git pull\n",
    "# !git checkout permanent_generation\n",
    "# !pip install pytorch_lightning\n",
    "# import sys\n",
    "# sys.path.append('Sk-DL2021-FinalProject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "repo_dir = Path().absolute().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87lwyusR35aH"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "\n",
    "import torch\n",
    "from data.split_step_generator import SplitStepGenerator, find_dataset_subdir\n",
    "\n",
    "GOOGLE_DRIVE = False\n",
    "\n",
    "if GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount(f'/content/drive')\n",
    "    root_dir = Path('/content/drive/drive/MyDrive/Sk-DL2021-Datasets')\n",
    "else:\n",
    "    root_dir = repo_dir.parent / 'generated_datasets'\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    \n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "CONFIG = f'{repo_dir}/configs/DebugDataset2D.yaml'\n",
    "BATCH_SIZE = 1\n",
    "GENERATE_TRAIN_BATCHES = 2\n",
    "GENERATE_VAL_BATCHES = 0\n",
    "GENERATE_TEST_BATCHES = 0\n",
    "\n",
    "if os.path.exists(CONFIG):\n",
    "    with open(CONFIG, 'r') as stream:\n",
    "        config_hparams = yaml.safe_load(stream)['data']['init_args']\n",
    "        config_hparams['data_source_type'] = 'generation'\n",
    "        config_hparams['load_dataset_root_path'] = None\n",
    "        config_hparams['batch_size'] = BATCH_SIZE\n",
    "        config_hparams['generate_n_train_batches'] = GENERATE_TRAIN_BATCHES\n",
    "        config_hparams['generate_n_val_batches'] = GENERATE_VAL_BATCHES\n",
    "        config_hparams['generate_n_test_batches'] = GENERATE_TEST_BATCHES\n",
    "else:\n",
    "    print('Config file cant be found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gu7yP7JL35aJ"
   },
   "outputs": [],
   "source": [
    "data_gen = SplitStepGenerator(**config_hparams)\n",
    "config_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xY0heg3q35aK",
    "outputId": "227cfde2-f301-4b0c-8a48-c4516a20d196"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import datetime \n",
    "\n",
    "NEW_DIR_NAME = 'high_nonlinearity'\n",
    "\n",
    "def create_destination(hparams, datasets_root, new_dir_name=None):\n",
    "    if new_dir_name is None:\n",
    "        new_dir = root_dir/datetime.datetime.now().strftime(\"%m-%d-%Y=%H-%M-%S\")\n",
    "    else:\n",
    "        new_dir = root_dir/new_dir_name\n",
    "    os.makedirs(new_dir)\n",
    "    assert not os.path.exists(f'{new_dir}/signal_hparams.yaml')\n",
    "    with open(f'{new_dir}/signal_hparams.yaml', 'w') as outfile:\n",
    "        yaml.dump(hparams, outfile, default_flow_style=False)\n",
    "    return new_dir\n",
    "    \n",
    "destination_root = find_dataset_subdir(data_gen.signal_hparams, root_dir)\n",
    "if destination_root is None:\n",
    "    destination_root = create_destination(data_gen.signal_hparams, root_dir, NEW_DIR_NAME)\n",
    "print('Destination: ', destination_root)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make folders structure\n",
    "def save_tensor(tensor, subdir):\n",
    "    if tensor is None:\n",
    "        print('Nothing to save', subdir)\n",
    "        return\n",
    "    if tensor.numel() == 0:\n",
    "        return\n",
    "    i = 0\n",
    "    while os.path.exists(subdir/f\"{i}.pt\"):\n",
    "        i += 1\n",
    "    destination_path = subdir/f\"{i}.pt\"\n",
    "    torch.save(torch.tensor([]), destination_path)\n",
    "    torch.save(tensor.clone(), destination_path)\n",
    "    \n",
    "type_subdirs = [destination_root/sub for sub in ['train', 'val', 'test']]\n",
    "for d in type_subdirs:\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73_7x0z435aO"
   },
   "outputs": [],
   "source": [
    "# generate and save\n",
    "N_REPEATS = 3\n",
    "\n",
    "for _ in range(N_REPEATS):\n",
    "    data_gen.prepare_data()\n",
    "    data_gen.setup()\n",
    "\n",
    "    save_tensor(data_gen.train, type_subdirs[0])\n",
    "    save_tensor(data_gen.val, type_subdirs[1])\n",
    "    save_tensor(data_gen.test, type_subdirs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_hparams['data_source_type'] = 'filesystem'\n",
    "config_hparams['load_dataset_root_path'] = root_dir\n",
    "config_hparams['batch_size'] = 2\n",
    "data_gen_load = SplitStepGenerator(**config_hparams)\n",
    "config_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5QFChzz35aR"
   },
   "outputs": [],
   "source": [
    "data_gen_load.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-61e63j935aU",
    "outputId": "5db3b2b3-c88f-4cfe-ba8a-aa043a8d4e43"
   },
   "outputs": [],
   "source": [
    "if data_gen_load.train is not None:\n",
    "    print(data_gen_load.train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "generate_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
