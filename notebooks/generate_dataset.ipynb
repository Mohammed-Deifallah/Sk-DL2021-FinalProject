{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "generate_dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_C9b6ZbbjHB"
      },
      "source": [
        "import os\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzfKIpz435Z-"
      },
      "source": [
        "# for colab\n",
        "!git clone https://github.com/laralex/Sk-DL2021-FinalProject\n",
        "repo_dir = Path().absolute()/'Sk-DL2021-FinalProject'\n",
        "%pushd Sk-DL2021-FinalProject\n",
        "!git pull\n",
        "!git checkout dataset_offline_generation\n",
        "!pip install pytorch_lightning\n",
        "import sys\n",
        "sys.path.append('Sk-DL2021-FinalProject')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiHxQlmybjHH"
      },
      "source": [
        "# for local\n",
        "# import sys\n",
        "# sys.path.append('..')\n",
        "# repo_dir = Path().absolute().parent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87lwyusR35aH"
      },
      "source": [
        "!pwd\n",
        "\n",
        "import torch\n",
        "from data.split_step_generator import SplitStepGenerator\n",
        "\n",
        "GOOGLE_DRIVE = True\n",
        "\n",
        "if GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(f'/content/drive')\n",
        "    root_dir = Path('/content/drive/MyDrive/Sk-DL2021-Datasets')\n",
        "else:\n",
        "    root_dir = repo_dir.parent / 'generated_datasets'\n",
        "if not os.path.exists(root_dir):\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    \n",
        "root_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZMafEeAbjHJ"
      },
      "source": [
        "import yaml\n",
        "CONFIG_NAME = '2d_nonlin0.50_no_disp'\n",
        "CONFIG = repo_dir/'configs'/f\"{CONFIG_NAME}.yaml\"\n",
        "BATCH_SIZE = 1\n",
        "GENERATE_TRAIN_BATCHES = 50\n",
        "GENERATE_VAL_BATCHES = 15\n",
        "GENERATE_TEST_BATCHES = 0\n",
        "\n",
        "if os.path.exists(CONFIG):\n",
        "    with open(CONFIG, 'r') as stream:\n",
        "        config_hparams = yaml.safe_load(stream)['data']['init_args']\n",
        "        config_hparams['data_source_type'] = 'generation'\n",
        "        config_hparams['load_dataset_root_path'] = None\n",
        "        config_hparams['batch_size'] = BATCH_SIZE\n",
        "        config_hparams['generate_n_train_batches'] = GENERATE_TRAIN_BATCHES\n",
        "        config_hparams['generate_n_val_batches'] = GENERATE_VAL_BATCHES\n",
        "        config_hparams['generate_n_test_batches'] = GENERATE_TEST_BATCHES\n",
        "else:\n",
        "    print('Config file cant be found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu7yP7JL35aJ"
      },
      "source": [
        "config_hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY0heg3q35aK"
      },
      "source": [
        "import yaml\n",
        "import datetime \n",
        "\n",
        "NEW_DIR_NAME = CONFIG_NAME\n",
        "\n",
        "def create_destination(hparams, datasets_root, new_dir_name=None):\n",
        "    if new_dir_name is None:\n",
        "        new_dir = root_dir/datetime.datetime.now().strftime(\"%m-%d-%Y=%H-%M-%S\")\n",
        "    else:\n",
        "        new_dir = root_dir/new_dir_name\n",
        "    os.makedirs(new_dir)\n",
        "    assert not os.path.exists(f'{new_dir}/signal_hparams.yaml')\n",
        "    with open(f'{new_dir}/signal_hparams.yaml', 'w') as outfile:\n",
        "        yaml.dump(hparams, outfile, default_flow_style=False)\n",
        "    return new_dir\n",
        "    \n",
        "destination_root = find_dataset_subdir(data_gen.signal_hparams, root_dir)\n",
        "if destination_root is None:\n",
        "    destination_root = create_destination(data_gen.signal_hparams, root_dir, NEW_DIR_NAME)\n",
        "print('Destination: ', destination_root)       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANsrvqLxbjHN"
      },
      "source": [
        "# make folders structure\n",
        "def save_tensor(tensor, subdir):\n",
        "    if tensor is None:\n",
        "        print('Nothing to save', subdir)\n",
        "        return\n",
        "    if tensor.numel() == 0:\n",
        "        return\n",
        "    i = 0\n",
        "    while os.path.exists(subdir/f\"{i}.pt\"):\n",
        "        i += 1\n",
        "    destination_path = subdir/f\"{i}.pt\"\n",
        "    torch.save(torch.tensor([]), destination_path)\n",
        "    torch.save(tensor.clone(), destination_path)\n",
        "    \n",
        "type_subdirs = [destination_root/sub for sub in ['train', 'val', 'test']]\n",
        "for d in type_subdirs:\n",
        "    os.makedirs(d, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73_7x0z435aO"
      },
      "source": [
        "# generate and save\n",
        "N_REPEATS = 20\n",
        "\n",
        "import time\n",
        "begin = time.time()\n",
        "for i in range(N_REPEATS):\n",
        "  data_gen = SplitStepGenerator(**config_hparams)\n",
        "  data_gen.prepare_data()\n",
        "  data_gen.setup()\n",
        "  print(data_gen.val.sum())\n",
        "  save_tensor(data_gen.train, type_subdirs[0])\n",
        "  save_tensor(data_gen.val, type_subdirs[1])\n",
        "  save_tensor(data_gen.test, type_subdirs[2])\n",
        "print(f'Time elapsed: {time.time() - begin}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j84w2kkabjHP"
      },
      "source": [
        "config_hparams['data_source_type'] = 'filesystem'\n",
        "config_hparams['load_dataset_root_path'] = root_dir\n",
        "config_hparams['batch_size'] = 20\n",
        "data_gen_load = SplitStepGenerator(**config_hparams)\n",
        "config_hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5QFChzz35aR"
      },
      "source": [
        "data_gen_load.prepare_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-61e63j935aU"
      },
      "source": [
        "if data_gen_load.train is not None:\n",
        "    print(data_gen_load.train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzXqYvoBbjHR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}